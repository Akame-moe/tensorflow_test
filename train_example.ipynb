{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,Conv2D,MaxPooling2D\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载所有图片数据，每个图片都resize成IMG_SIZE大小。整体打乱并将数据集写入文件，方便下次直接读取数据集而不是遍历文件夹一个一个的读取图片。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "impath = ['datasets/English/Img/Sample%03d' % (x+1) for x in range(10)]\n",
    "IMG_SIZE = 40\n",
    "data_set = []\n",
    "def load_data():\n",
    "    for p in impath:\n",
    "        label = impath.index(p)\n",
    "        for pp in os.listdir(p):\n",
    "            try:\n",
    "                im = cv2.imread(os.path.join(p,pp),cv2.IMREAD_GRAYSCALE)\n",
    "                new_im = cv2.resize(im,(IMG_SIZE,IMG_SIZE))\n",
    "                data_set.append([new_im,label])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "            \n",
    "load_data()\n",
    "random.shuffle(data_set)\n",
    "with open('data_set.pickle','wb') as f:\n",
    "    pickle.dump(data_set,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将直接从文件中把数据集恢复。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 3, 1, 1, 3, 2, 5, 8, 5, 9]\n"
     ]
    }
   ],
   "source": [
    "def load_train_data():\n",
    "    images = []\n",
    "    labels = []\n",
    "    with open('data_set.pickle','rb') as f:\n",
    "        pickle_data = pickle.load(f)\n",
    "    for x in pickle_data:\n",
    "        images.append(x[0])\n",
    "        labels.append(x[1])\n",
    "    return images,labels\n",
    "X_train,Y_train = load_train_data()\n",
    "print(Y_train[:10])\n",
    "#因为读取出来的数据是列表。所以需要用numpy把其reshape成numpy的数组\n",
    "#X_train = np.reshape(X_train,(-1,IMG_SIZE,IMG_SIZE))\n",
    "X_train = np.reshape(X_train,(-1,IMG_SIZE,IMG_SIZE,1))\n",
    "Y_train = np.reshape(Y_train,(-1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取前十个数据验证一下是否正确"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 3 1 1 3 2 5 8 5 9]\n"
     ]
    }
   ],
   "source": [
    "#for x in X_train[:3]:\n",
    "#    plt.figure()\n",
    "#    plt.imshow(x)\n",
    "    \n",
    "print(Y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将0-255的数据缩放到0-1之间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 440 samples, validate on 110 samples\n",
      "Epoch 1/10\n",
      "440/440 [==============================] - 6s 14ms/step - loss: 2.3074 - acc: 0.1364 - val_loss: 2.2069 - val_acc: 0.2364\n",
      "Epoch 2/10\n",
      "440/440 [==============================] - 6s 13ms/step - loss: 1.9045 - acc: 0.3568 - val_loss: 1.5990 - val_acc: 0.5182\n",
      "Epoch 3/10\n",
      "440/440 [==============================] - 6s 13ms/step - loss: 1.2251 - acc: 0.6136 - val_loss: 0.9922 - val_acc: 0.7000\n",
      "Epoch 4/10\n",
      "440/440 [==============================] - 6s 13ms/step - loss: 0.8894 - acc: 0.7182 - val_loss: 0.8486 - val_acc: 0.7455\n",
      "Epoch 5/10\n",
      "440/440 [==============================] - 6s 14ms/step - loss: 0.5856 - acc: 0.8341 - val_loss: 0.7758 - val_acc: 0.7545\n",
      "Epoch 6/10\n",
      "440/440 [==============================] - 6s 13ms/step - loss: 0.3714 - acc: 0.8818 - val_loss: 0.7508 - val_acc: 0.7727\n",
      "Epoch 7/10\n",
      "440/440 [==============================] - 6s 13ms/step - loss: 0.2670 - acc: 0.9295 - val_loss: 0.7335 - val_acc: 0.7636\n",
      "Epoch 8/10\n",
      "440/440 [==============================] - 6s 13ms/step - loss: 0.1429 - acc: 0.9682 - val_loss: 0.6895 - val_acc: 0.8091\n",
      "Epoch 9/10\n",
      "440/440 [==============================] - 6s 13ms/step - loss: 0.0840 - acc: 0.9909 - val_loss: 0.7606 - val_acc: 0.8091\n",
      "Epoch 10/10\n",
      "440/440 [==============================] - 6s 13ms/step - loss: 0.0599 - acc: 0.9864 - val_loss: 0.8284 - val_acc: 0.8182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f43e05eb748>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#配置tensorboard\n",
    "filename = 'mymodel-{}.ckpt'.format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(filename))\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(128,(3,3),input_shape=(IMG_SIZE,IMG_SIZE,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64,(3,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "#validation_split：测试数据和训练数据的分离比例。因为这两个其实是一样的数据，之前需要手动传入测试集，\n",
    "#现在只需要定义一下比例，tensorflow自动会按比例随机选取测试样本\n",
    "model.fit(X_train,Y_train,batch_size=10,validation_split=0.2,epochs=10,callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试前十个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "5\n",
      "8\n",
      "5\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(X_train[:10])\n",
    "for p in pred:\n",
    "    print(np.argmax(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('CNN-128-64-64-with-98-acc.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
